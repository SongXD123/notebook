

# Attention Pool :  Nadaraya-Watson 核回归

queries(volitional cues)和keys(nonvolitional cues)相互作用导致attention pooling，attention pooling有选择地聚合一些values(sensory inputs)来产生输出



**简单地从回归问题开始考虑，由${(x_1,y_1),...,(x_n,y_n)}$学习$\hat{y}=f(x)$**

## 2.1 **Average Pooling**

最”笨“地回归方法：使用average pooling对整个训练集输出进行平均
$$
f(x)=\frac{1}{n}\sum_{i=1}^{n}y_i\tag{1}
$$

## 2.2 **Nonparametric Attention Pooling**

显然，average pooling忽略了输入$x_i$，另一个方法是Nadaraya和Waston提出地根据输入位置对$y_i$进行加权
$$
f(x)=\sum_{i=1}^{n}\frac{K(x-x_i)}{\sum_{j=1}^{n}K(x-x_j)}y_i\tag{2}
$$
其中，$K$是核函数。这种估计方法称为Nadaraya-Watson核回归。从注意力角度，可以将上式写作更加通用地attention pooling的形式：
$$
f(x)=\sum_{i=1}^{n}\alpha(x,x_i)y_y\tag{3}
$$
其中，$x$是query，$(x_i,y_i)$是key-value对。这里的attention pooling是对$y_i$的加权平均。attention weight $\alpha(x,x_i)$被分给对应的值$y_i$，它是基于query $x$和key $x_i$。

对于任意的query，它在所有key-value对上的attention weight是一个概率分布：非负且和为1：

为了引入attention pooling，先看以下Gaussian Kernel的定义：
$$
K(u)=\frac{1}{\sqrt{2\pi}}exp(-\frac{u^2}{2})\tag{4}
$$
将高斯核函数带入(2）和（3），得：
$$
f(x)=\sum_{i=1}^{n}\alpha{(x_i,y_i)}y_i\\=\sum_{i=1}^{n}\frac{exp(-\frac{1}{2}(x-x_i)^2)}{\sum_{j=1}^{n}exp(-\frac{1}{2}(x-x_i)^2)}\\\sum_{i=1}^{n}softmax(-\frac{1}{2}(x-x_i)^2)y_i\tag{5}
$$
**在上式中，键（key）$x_i$更接近给定查询（query）$x$，就会因为对应的值（value）$y_i$被分配更大的attention weight从而得到更多的attention**

值得注意的是，Nadaraya-Watson核回归是一个**非参数模型**，因此（5）是**非参数注意力集中**的一个例子。

* x_train包含keys
* y_train包含values
* x_test包含query



## 2.3 Parametric Attention Pooling

非参数的Nadaraya-Watson核回归具有一致性优势：**给定足够的数据，该模型收敛到最优解**

然而，我们可以很容易地将**可学习地参数**融入attention pooling

例如，与（5）不同的是，以下query $x$和key $x_i$之间的距离乘以可学习参数$w$：
$$
f(x)=\sum_{i=1}^{n}\alpha(x,x_i)y_y\\=\sum_{i=1}{n}\frac{exp(-\frac{1}{2}((x-x_i)w)^2)}{\sum_{j=1}^{n}exp(-\frac{1}{2}((x-x_i)w)^2)}y_i\\
=\sum_{i=1}^{n}softmax(-\frac{1}{2}((x-x_i)w)^2)y_i\tag{6}
$$
通过训练模型学习（6）中的参数

#### 2.3.1 批量矩阵乘法（Batch Matrix Multiplication）

为了更高效计算小批量的注意力，我们可以利用深度学习框架提供的批量矩阵乘法工具。

假设第一个批量包含$n$个大小$a \times b$的矩阵$\pmb{X_1},...,\pmb{X_n}$，和第二个小批量包含$n$个大小为$b \times c$的矩阵$\pmb{X_1Y_1},...,\pmb{X_nY_n}$。因此，给定两个形状分别为$(n,a,b)$和$(n,b,c)$的张量，它们批量乘法结果形状为$(n,a,c)$

```
X = torch.ones((2, 1, 4))
Y = torch.ones((2, 4, 6))
torch.bmm(X, Y).shape
```

#### 2.3.2 定义模型

基于参数化attention pooling的Nadaraya-Watson核回归的参数化版本

```python
class NWKernelRegression(nn.Module):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.w = nn.Parameter(torch.rand((1, ), requires_grad=True))

    def forward(self, queries, keys, values):
        # Shape of the output `queries` and `attention_weights`:
        # (no. of queries, no. of key-value pairs)
        queries = queries.repeat_interleave(keys.shape[1]).reshape(
            (-1, keys.shape[1]))
        self.attention_weights = nn.functional.softmax(
            -((queries - keys) * self.w)**2 / 2, dim=1)
        # Shape of `values`: (no. of queries, no. of key-value pairs)
        return torch.bmm(self.attention_weights.unsqueeze(1),
                         values.unsqueeze(-1)).reshape(-1)
```



## 2.4 总结

* Nadaraya-Watson核回归的attention pooling是对训练集输出的加权平均。从注意力角度，注意力权重基于key和query的函数，被分配到不同value





