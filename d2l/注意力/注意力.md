![image-20210306200229435](D:\ProgramData\gitRepos\notebook\d2l\images\image-20210306200229435.png)

* 主体分别通过nonvolitional cues 和 volitional cues来转移注意力。前者基于物体显著性，后者依赖任务
* keys：nonvolitional cues，对应于显著的物体和感官，会影响人的选择
* query：volitional cues，对应处于自主和认知的选择
* values：对应环境的感官输入
* Attention mechanisms bias selection over values (sensory inputs) via attention pooling, which incorporates queries (volitional cues) and keys (nonvolitional cues). Keys and values are paired.
* 除上图外，还有许多注意力机制地设计，如通过强化学习方法训练的non-differentiable attention model 

# 注意力可视化

为可视化注意力权重，定义show_heatmaps函数，输入为 (number of rows for display, number of columns for display, number of queries, number of keys)矩阵

# Attention Pool :  Nadaraya-Watson 核回归

queries(volitional cues)和keys(nonvolitional cues)相互作用导致attention pooling，attention pooling有选择地聚合一些values(sensory inputs)来产生输出



#### 简单地从回归问题开始考虑，由${(x_1,y_1),...,(x_n,y_n)}$学习$\hat{y}=f(x)$

1. **Average Pooling**

   最”笨“地回归方法：使用average pooling对整个训练集输出进行平均
   $$
   f(x)=\frac{1}{n}\sum_{i=1}^{n}y_i\tag{1}
   $$
   

2. **Nonparametric Attention Pooling**

   显然，average pooling忽略了输入$x_i$，另一个方法是Nadaraya和Waston提出地根据输入位置对$y_i$进行加权
   $$
   f(x)=\sum_{i=1}^{n}\frac{K(x-x_i)}{\sum_{j=1}^{n}K(x-x_j)}y_i\tag{2}
   $$
   其中，$K$是核函数。这种估计方法称为Nadaraya-Watson核回归。从注意力角度，可以将上式写作更加通用地attention pooling的形式：
   $$
   f(x)=\sum_{i=1}^{n}\alpha(x,x_i)y_y\tag{3}
   $$
   其中，$x$是query，$(x_i,y_i)$是key-value对。这里的attention pooling是对$y_i$的加权平均。attention weight $\alpha(x,x_i)$被分给对应的值$y_i$，它是基于query $x$和key $x_i$。

   对于任意的query，它在所有key-value对上的attention weight是一个概率分布：非负且和为1：

   为了引入attention pooling，先看以下Gaussian Kernel的定义：
   $$
   K(u)=\frac{1}{\sqrt{2\pi}}exp(-\frac{u^2}{2})\tag{4}
   $$
   将高斯核函数带入(2）和（3），得：
   $$
   f(x)=\sum_{i=1}^{n}\alpha{(x_i,y_i)}y_i\\=\sum_{i=1}^{n}\frac{exp(-\frac{1}{2}(x-x_i)^2)}{\sum_{j=1}^{n}exp(-\frac{1}{2}(x-x_i)^2)}\\\sum_{i=1}^{n}softmax(-\frac{1}{2}(x-x_i)^2)y_i\tag{5}
   $$
   **在上式中，键（key）$x_i$更接近给定查询（query）$x$，就会因为对应的值（value）$y_i$被分配更大的attention weight从而得到更多的attention**

   值得注意的是，Nadaraya-Watson核回归是一个**非参数模型**，因此（5）是非参数注意力集中的一个例子。

3. 