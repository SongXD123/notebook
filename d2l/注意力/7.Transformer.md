不同于早期自注意力模型（仍然**依赖RNNs来进行输入表示**），Transformer仅基于注意力机制而不依赖卷积层或RNN。

## 7.1 模型

作为一种encoder-decoder结构，Transformer的整体架构如图所示。

![image-20210311173423056](D:\ProgramData\gitRepos\notebook\d2l\注意力\images\image-20210311173423056.png)

Transformer由一个**encoder**和一个**decoder**组成。不同于Bahdanau注意力在seq2seq中的应用，输入（source）和输出（target）序列嵌入后，加上了位置编码，然后再输入由自注意力堆叠的encoder和decoder。

Transformer的encoder是由多个相同层堆叠而成，其中每个层有两个子层（记为$sublayer$)。
**==encoder==**

* 第一个是**multi-head self-attention pooling**
  * queries、keys和values全部来自**前一个encoder层的输出**
  * 结果是，Transformer的encoder输出$d$维向量表示输入序列每个位置
* 第二个是 **positionwise feed-forward network**
* 残差网络在两个子层中都有应用
  * 在Transformer中，在序列任意位置的任意输入$\pmb{x}\in{\mathbb{R}^d}$，要求$sublayer(\pmb{x})\in\pmb{R}^d$，这样残差连接才是合理的。
  * 与残差连接相加后，接着一个层归一化（layer normalization）。



**==decoder==**

decoder同样是由多个相同的带残差连接和层归一化的层组成。除了encoder中的两个子层外，decoder加入了第三个子层，称为**encoder-decoder attention**，其位置在**multi-head self-attention pooling**和 **positionwise feed-forward network**之间。

* **encoder-decoder attention**：
  * 该层中，**queries**来自**前一个decoder层的输出**，**keys和values**来自**Transformer encoder的输出**
* **decoder self-attention**
  * queries、keys和values均来自**前一个decoder层的输出**
  * 然而，decoder中的每个位置只允许关注decoder中**该位置之前的位置**。
  * masked attention保留了自回归特性，保证预测只依赖已经被生成的token

## 7.2 Positionwise Feed-Forward Networks

该层将序列所有位置的表达用相同的MLP转换，实现时，输入**X(batch_size,num_steps,num_hiddens/features)**，经过两层MLP转换为**X(batch_size,num_steps,ffn_num_outputs)**

## 7.3 残差连接和层归一化

图中**“add&norm"**是一个残差连接紧跟层归一化。两者都是高效的深度结构的关键。

回顾批量归一化，在一个小批量对样本进行了平移和缩放。层归一化与批量归一化基本相同，主要区别在于层归一化会跨特征进行归一化（**批量归一化对每个神经元分别归一化，层归一化对所有神经元进行归一化**）。尽管批量归一化在CV中应用广泛，批量归一化在自然语言处理中不如层归一化高效，因为**输入通常长度可变**

同时，残差连接要求两个输入**==形状相同==**。

```python
class AddNorm(nn.Module):
    def __init__(self, normalized_shape, dropout, **kwargs):# Normalized_shape is input.size()[1:]
        super(AddNorm, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)
        self.ln = nn.LayerNorm(normalized_shape)

    def forward(self, X, Y):
        return self.ln(self.dropout(Y) + X)
```



## 7.4 Encoder

两个子层：**multi-head self-attention**和**positionwise feed-forward networks**，其中每个子层最后都是残差连接+层归一化

* 定义EncoderBlock
* 将多个EncoderBlock堆叠起来，（这里使用固定位置编码，值总是在-1到1之间，**我们将学习的输入嵌入的值与嵌入维度的平方根相乘**，再对embedding求和并进行位置编码（Decoder同样如此）

## 7.5 Decoder

* **DecoderBlock**块
  * decoder self-attention
    * queries、keys和values均来自**前一个decoder层的输出**
    * 当训练seq2seq模型时，所有位置（时间步）的token都已知，
    * 然而，在预测时，只有已经生成的token可以用于decoder self-attention
    * 为保证自回归性，masked self-attention设置了**dec_valid_lens**，所以所有query只注意该时间步之前的位置
  * encoder-decoder attention
  * positionwise feed-forward networks
  * 每一个子层都有残差连接和层归一化
  * 为方便缩放点积运算，decoder的num_hiddens与encoder的相同





